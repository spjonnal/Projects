{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\ntrain_data = pd.read_csv('/kaggle/input/toxicity/train.csv')\ntrain_data['labels'] = train_data[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values.tolist()\ntest_data = pd.read_csv('/kaggle/input/toxicity/test.csv')\ntest_labels = pd.read_csv('/kaggle/input/toxicity/test_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:14:13.982684Z","iopub.execute_input":"2023-07-17T00:14:13.983039Z","iopub.status.idle":"2023-07-17T00:14:16.246539Z","shell.execute_reply.started":"2023-07-17T00:14:13.983008Z","shell.execute_reply":"2023-07-17T00:14:16.245461Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"#test_labels = test_labels.drop('id',axis =1)\nimport pandas as pd\nimport torch\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import DistilBertTokenizer, DistilBertModel\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weights = [1,2,1,1,1,1]\ntoxicity = []\nfor i in train_data['labels'].values:\n    weighted = 0\n    for j in range(len(i)):\n        weighted += i[j] * weights[j]\n        w_s = weighted/sum(weights)\n    toxicity.append(w_s)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:14:29.754357Z","iopub.execute_input":"2023-07-17T00:14:29.755045Z","iopub.status.idle":"2023-07-17T00:14:30.436473Z","shell.execute_reply.started":"2023-07-17T00:14:29.755010Z","shell.execute_reply":"2023-07-17T00:14:30.435473Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"toxicity_reshaped = []\nfor i in toxicity:\n    if(i ==0):\n        toxicity_reshaped.append(0)\n    else:\n        toxicity_reshaped.append(1)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:14:30.438639Z","iopub.execute_input":"2023-07-17T00:14:30.439021Z","iopub.status.idle":"2023-07-17T00:14:30.480911Z","shell.execute_reply.started":"2023-07-17T00:14:30.438977Z","shell.execute_reply":"2023-07-17T00:14:30.480021Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"count_zero, count_one = 0, 0\nfor i in toxicity_reshaped:\n    if(i == 0):\n        count_zero += 1\n    else:\n        count_one += 1\nprint(count_zero,count_one)\n#157209 2362 0.5 threshold\n#153145 6426 0.3 threshold\n#149706 9865 0.2 threshold\n#143346 16225 0 threshold\nprint(toxicity_reshaped)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -q transformers\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:14:44.889984Z","iopub.execute_input":"2023-07-17T00:14:44.890713Z","iopub.status.idle":"2023-07-17T00:14:58.205886Z","shell.execute_reply.started":"2023-07-17T00:14:44.890675Z","shell.execute_reply":"2023-07-17T00:14:58.204623Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"###### Preprocessing - removing https links and ip address #####\nimport re\nremoved_hyperlinks_training = []\nremoved_hyperlinks_testing = []\npattern = re.compile(r\"(http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+)|(\\d{1,3}\\.\\d{1,3}\\.\\d{1,3}\\.\\d{1,3})\")\n\nfor val in train_data['comment_text'].values:\n  temp = re.sub(pattern,'',val)\n  removed_hyperlinks_training.append(temp)\n\nfor val in test_data['comment_text'].values:\n  temp = re.sub(pattern,'',val)\n  removed_hyperlinks_testing.append(temp)\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:19:33.548352Z","iopub.execute_input":"2023-07-17T00:19:33.548741Z","iopub.status.idle":"2023-07-17T00:19:51.101909Z","shell.execute_reply.started":"2023-07-17T00:19:33.548710Z","shell.execute_reply":"2023-07-17T00:19:51.100916Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"#### Preprocessing - removing html tags and code ######\ntags = re.compile(r\"\\|.*\")\nremoved_html_tags_code_training = []\nremoved_html_tags_code_testing = []\nfor val in removed_hyperlinks_training:\n  temp = re.sub(tags,'',val)\n  removed_html_tags_code_training.append(temp)\n\nfor val in removed_hyperlinks_testing:\n  temp = re.sub(tags,'',val)\n  removed_html_tags_code_testing.append(temp)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:19:51.190705Z","iopub.execute_input":"2023-07-17T00:19:51.191173Z","iopub.status.idle":"2023-07-17T00:19:51.915288Z","shell.execute_reply.started":"2023-07-17T00:19:51.191131Z","shell.execute_reply":"2023-07-17T00:19:51.914158Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"### Preprocessing - removing numbers, digits and punctuations except “”’, “.”, “!”, “?”. ######\nremoved_numbers_punctuations_training = []\nremoved_numbers_punctuations_testing = []\nfor val in removed_html_tags_code_training:\n    temp = re.sub(r\"\\d+\", \"\", val)\n    temp = re.sub(r\"[^\\w\\s''-]\", \"\",temp) #r\"[^\\w\\s.!?''-]\"\n    removed_numbers_punctuations_training.append(temp)\n\nfor val in removed_html_tags_code_testing:\n    temp = re.sub(r\"\\d+\", \"\", val)\n    temp = re.sub(r\"[^\\w\\s''-]\", \"\",temp) #r\"[^\\w\\s.!?''-]\"\n    removed_numbers_punctuations_testing.append(temp)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:19:51.917550Z","iopub.execute_input":"2023-07-17T00:19:51.917965Z","iopub.status.idle":"2023-07-17T00:20:02.371004Z","shell.execute_reply.started":"2023-07-17T00:19:51.917930Z","shell.execute_reply":"2023-07-17T00:20:02.369900Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"## Removing Stopwords\nstopwords_english = ['then', 'only', 'shouldn', 'themselves', \"hadn't\", 'off', 'm', \"isn't\", 'because', 'couldn', 'most', 'any', 'not', 'what', 'yours', 'very', 'them', 'there', \"don't\", \"you're\", 'that', \"wouldn't\", 'd', 'did', 'further', 'own', 'needn', 'they', 'my', 'o', 'mustn', 'didn', 'as', 'doesn', \"she's\", 'ain', 'will', 'too', \"hasn't\", 'to', 'a', 'aren', 'i', 'who', 'those', 'or', 'while', 'here', 'no', \"didn't\", 'during', 'until', 'before', 'yourselves', 'same', 'itself', 'you', 'am', \"doesn't\", 'these', 'll', 'which', 'was', 'mightn', 'more', \"it's\", 'do', 'all', 'has', 'won', 'once', 'isn', 'had', 'haven', 'at', 'yourself', 'herself', 'about', 'how', 'ourselves', 'this', \"you'll\", 'theirs', 'were', \"mightn't\", 'few', 'being', 'having', 'out', 'wouldn', 'now', 'him', 'by', 'through', 'against', 'nor', 'of', 'should', 'some', \"haven't\", 'up', 'when', \"aren't\", 'hadn', \"that'll\", 'so', 'ours', 'does', 'weren', 'he', 'and', 'into', 'be', 'from', \"weren't\", 'but', 'wasn', \"couldn't\", 'our', 'can', 'after', 'his', 'other', 'their', 'where', 'with', 'are', \"needn't\", 'its', 'than', 'her', 'she', 'over', \"shan't\", 've', 'down', 'on', 'again', 't', 'below', 'in', 'have', \"wasn't\", 'y', 'for', 'hasn', 'between', \"mustn't\", 'himself', 'shan', 'the', 'an', 'both', 'is', 'doing', 'just', \"should've\", 'your', 'don', 'myself', 'above', 'such', \"you've\", 'it', 'been', 'me', \"won't\", \"you'd\", 'each', 's', 'we', 're', 'hers', 'ma', 'why', \"shouldn't\", 'whom', 'under', 'if']","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:20:02.372409Z","iopub.execute_input":"2023-07-17T00:20:02.372804Z","iopub.status.idle":"2023-07-17T00:20:02.386282Z","shell.execute_reply.started":"2023-07-17T00:20:02.372768Z","shell.execute_reply":"2023-07-17T00:20:02.383912Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"######### tokenize the corpus ###############\ntokenized_cleaned_comments_training = []\ntokenized_cleaned_comments_testing = []\nfor i in removed_numbers_punctuations_training:\n\n    temp_text = re.split('(\\W)', i)\n    temp_text = [j for j in temp_text if j not in r\"[^a-zA-Z']+|[0-9]+|[.]+|\\s+ \" ]\n    temp_text = [j for j in temp_text if j.lower() not in stopwords_english]\n    tokenized_cleaned_comments_training.append(temp_text)\n\nfor i in removed_numbers_punctuations_testing:\n\n    temp_text = re.split('(\\W)', i)\n    temp_text = [j for j in temp_text if j not in r\"[^a-zA-Z']+|[0-9]+|[.]+|\\s+ \" ]\n    temp_text = [j for j in temp_text if j.lower() not in stopwords_english]\n    tokenized_cleaned_comments_testing.append(temp_text)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:20:02.389137Z","iopub.execute_input":"2023-07-17T00:20:02.389505Z","iopub.status.idle":"2023-07-17T00:21:01.492477Z","shell.execute_reply.started":"2023-07-17T00:20:02.389461Z","shell.execute_reply":"2023-07-17T00:21:01.491486Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"cleaned_trainig_data = [' '.join(tokens).replace('\\n',\"\") for tokens in tokenized_cleaned_comments_training]\ncleaned_trainig_test = [' '.join(tokens).replace('\\n',\"\") for tokens in tokenized_cleaned_comments_testing]","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:26:16.360556Z","iopub.execute_input":"2023-07-17T00:26:16.361242Z","iopub.status.idle":"2023-07-17T00:26:17.023415Z","shell.execute_reply.started":"2023-07-17T00:26:16.361206Z","shell.execute_reply":"2023-07-17T00:26:17.022422Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"label_list, comment_list = [], []\nfor comment, label in zip(cleaned_trainig_data, train_data[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values):\n    label = label.tolist()\n    comment_list.append(comment)\n    label_list.append(label)\n\nfinal_cleaned_dataset_train = pd.DataFrame({'Comment': comment_list, 'Label': label_list})","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:36:57.798051Z","iopub.execute_input":"2023-07-17T00:36:57.798421Z","iopub.status.idle":"2023-07-17T00:36:58.816769Z","shell.execute_reply.started":"2023-07-17T00:36:57.798390Z","shell.execute_reply":"2023-07-17T00:36:58.815726Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"final_cleaned_dataset_train =  final_cleaned_dataset_train.sample(frac = 1)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:42:10.701405Z","iopub.execute_input":"2023-07-17T00:42:10.702556Z","iopub.status.idle":"2023-07-17T00:42:10.731939Z","shell.execute_reply.started":"2023-07-17T00:42:10.702493Z","shell.execute_reply":"2023-07-17T00:42:10.730980Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"maxilen = 128\ntrain_batch = 32\nval_batch = 32\ndev = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nprint(dev)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:43:18.141241Z","iopub.execute_input":"2023-07-17T00:43:18.141646Z","iopub.status.idle":"2023-07-17T00:43:18.147614Z","shell.execute_reply.started":"2023-07-17T00:43:18.141596Z","shell.execute_reply":"2023-07-17T00:43:18.146636Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"class ToxicComments_dataset(Dataset):\n\n    def __init__(self, df, token, max_len, new_data=False):\n        self.tokenizer = token\n        self.data = df\n        self.text = df.Comment\n        self.new_data = new_data\n        \n        if not new_data:\n            self.targets = self.data.Label\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n        out = {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n        }\n        \n        if not self.new_data:\n            out['targets'] = torch.tensor(self.targets[index], dtype=torch.float)\n\n        return out\n","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:43:56.371752Z","iopub.execute_input":"2023-07-17T00:43:56.372450Z","iopub.status.idle":"2023-07-17T00:43:56.384765Z","shell.execute_reply.started":"2023-07-17T00:43:56.372413Z","shell.execute_reply":"2023-07-17T00:43:56.383648Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"train_size = 0.9\n\ntrain_data_complete = final_cleaned_dataset_train.sample(frac=train_size, random_state=123)\nval_data_complete = final_cleaned_dataset_train.drop(train_data_complete.index).reset_index(drop=True)\ntrain_data_complete = train_data_complete.reset_index(drop=True)\n\n\nprint(\"Orig Dataset: {}\".format(train_data.shape))\nprint(\"Training Dataset: {}\".format(train_data_complete.shape))\nprint(\"Validation Dataset: {}\".format(val_data_complete.shape))\n\ndistil_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', truncation=True, do_lower_case=True)\ntraining_set = ToxicComments_dataset(train_data_complete, distil_tokenizer, maxilen)\nval_set = ToxicComments_dataset(val_data_complete, distil_tokenizer, maxilen)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:43:57.025026Z","iopub.execute_input":"2023-07-17T00:43:57.025732Z","iopub.status.idle":"2023-07-17T00:43:57.170838Z","shell.execute_reply.started":"2023-07-17T00:43:57.025689Z","shell.execute_reply":"2023-07-17T00:43:57.169805Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"Orig Dataset: (159571, 9)\nTraining Dataset: (143614, 2)\nValidation Dataset: (15957, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': 32,\n                'shuffle': True,\n                'num_workers': 8\n                }\n\nval_params = {'batch_size': 32,\n               'shuffle': True,\n               'num_workers': 8\n                }\n\ntrain_data_load = DataLoader(training_set, **train_params)\nval_data_load = DataLoader(val_set, **val_params)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:45:43.098444Z","iopub.execute_input":"2023-07-17T00:45:43.098847Z","iopub.status.idle":"2023-07-17T00:45:43.109674Z","shell.execute_reply.started":"2023-07-17T00:45:43.098815Z","shell.execute_reply":"2023-07-17T00:45:43.108601Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"class DistilBERTClass(torch.nn.Module):\n    def __init__(self):\n        super(DistilBERTClass, self).__init__()\n        \n        self.bert = DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Linear(768, 768),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.1),\n            torch.nn.Linear(768, 6)\n        )\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n        hidden_state = output_1[0]\n        out = hidden_state[:, 0]\n        out = self.classifier(out)\n        return out\n\ndistil_model = DistilBERTClass()\ndistil_model.to(dev);","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:47:51.161279Z","iopub.execute_input":"2023-07-17T00:47:51.161670Z","iopub.status.idle":"2023-07-17T00:47:51.941335Z","shell.execute_reply.started":"2023-07-17T00:47:51.161637Z","shell.execute_reply":"2023-07-17T00:47:51.940354Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']\n- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params=distil_model.parameters(), lr=1e-5)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:47:59.328973Z","iopub.execute_input":"2023-07-17T00:47:59.329334Z","iopub.status.idle":"2023-07-17T00:47:59.335633Z","shell.execute_reply.started":"2023-07-17T00:47:59.329304Z","shell.execute_reply":"2023-07-17T00:47:59.334308Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"def train(epoch):\n    model.train()\n    \n    for i, data in tqdm(enumerate(train_data_load, 0)):\n        ids = data['ids'].to(dev, dtype=torch.long)\n        mask = data['mask'].to(dev, dtype=torch.long)\n        token_type_ids = data['token_type_ids'].to(dev, dtype=torch.long)\n        targets = data['targets'].to(dev, dtype=torch.float)\n\n        outputs = distil_model(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = torch.nn.functional.binary_cross_entropy_with_logits(outputs, targets)\n        \n        if i % 2500 == 0:\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:49:43.860038Z","iopub.execute_input":"2023-07-17T00:49:43.860410Z","iopub.status.idle":"2023-07-17T00:49:43.870094Z","shell.execute_reply.started":"2023-07-17T00:49:43.860375Z","shell.execute_reply":"2023-07-17T00:49:43.868682Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"for i in range(3):\n    train(i)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T00:49:44.240816Z","iopub.execute_input":"2023-07-17T00:49:44.241146Z","iopub.status.idle":"2023-07-17T02:03:00.184907Z","shell.execute_reply.started":"2023-07-17T00:49:44.241118Z","shell.execute_reply":"2023-07-17T02:03:00.183076Z"},"trusted":true},"execution_count":65,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss:  0.7104593515396118\n","output_type":"stream"},{"name":"stderr","text":"2501it [13:36,  3.07it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 0, Loss:  0.06430947780609131\n","output_type":"stream"},{"name":"stderr","text":"4488it [24:24,  3.06it/s]\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n0it [00:00, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Loss:  0.031932149082422256\n","output_type":"stream"},{"name":"stderr","text":"2501it [13:36,  3.07it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 1, Loss:  0.04716680571436882\n","output_type":"stream"},{"name":"stderr","text":"4488it [24:24,  3.06it/s]\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n1it [00:00,  1.29it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Loss:  0.008854628540575504\n","output_type":"stream"},{"name":"stderr","text":"2501it [13:36,  3.06it/s]","output_type":"stream"},{"name":"stdout","text":"Epoch: 2, Loss:  0.004425479099154472\n","output_type":"stream"},{"name":"stderr","text":"4488it [24:25,  3.06it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"test_labels = pd.read_csv('/kaggle/input/toxicity/test_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:07:45.542972Z","iopub.execute_input":"2023-07-17T02:07:45.543355Z","iopub.status.idle":"2023-07-17T02:07:45.721146Z","shell.execute_reply.started":"2023-07-17T02:07:45.543319Z","shell.execute_reply":"2023-07-17T02:07:45.720076Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"test_labels = test_labels.drop('id',axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:08:10.424282Z","iopub.execute_input":"2023-07-17T02:08:10.424673Z","iopub.status.idle":"2023-07-17T02:08:10.433983Z","shell.execute_reply.started":"2023-07-17T02:08:10.424639Z","shell.execute_reply":"2023-07-17T02:08:10.433017Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"comment_list_test = []\nfor comment in (cleaned_trainig_test):\n    #label = label.tolist()\n    comment_list_test.append(comment)\n    #label_list.append(label)\n\nfinal_cleaned_dataset_test = pd.DataFrame({'Comment': comment_list_test})","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:10:40.863951Z","iopub.execute_input":"2023-07-17T02:10:40.864315Z","iopub.status.idle":"2023-07-17T02:10:40.943099Z","shell.execute_reply.started":"2023-07-17T02:10:40.864285Z","shell.execute_reply":"2023-07-17T02:10:40.942137Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"test_params = {'batch_size': 32,\n               'shuffle': True,\n               'num_workers': 8\n                }\n\ntest_set = ToxicComments_dataset(final_cleaned_dataset_test, distil_tokenizer, maxilen, new_data=True)\ntest_loader = DataLoader(test_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:12:05.226805Z","iopub.execute_input":"2023-07-17T02:12:05.227196Z","iopub.status.idle":"2023-07-17T02:12:05.233880Z","shell.execute_reply.started":"2023-07-17T02:12:05.227163Z","shell.execute_reply":"2023-07-17T02:12:05.232883Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\n","output_type":"stream"}]},{"cell_type":"code","source":"predictions = []\n\ndef test(epoch):\n    model.eval()\n    \n    with torch.inference_mode():\n    \n        for i, text in tqdm(enumerate(test_loader, 0)):\n\n\n            ids = text['ids'].to(dev, dtype=torch.long)\n            mask = text['mask'].to(dev, dtype=torch.long)\n            token_type_ids = text['token_type_ids'].to(dev, dtype=torch.long)\n            outputs = model(ids, mask, token_type_ids)\n            probaility_outputs = torch.sigmoid(outputs)\n\n            predictions.append(probaility_outputs)\n            \n            \n    return probaility_outputs\nprobaility_outputs = test(distil_model)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:14:52.994959Z","iopub.execute_input":"2023-07-17T02:14:52.995393Z","iopub.status.idle":"2023-07-17T02:24:06.465946Z","shell.execute_reply.started":"2023-07-17T02:14:52.995361Z","shell.execute_reply":"2023-07-17T02:24:06.464622Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n0it [00:00, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n  warnings.warn(\n4787it [09:12,  8.66it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"torch.save(distil_model,'final_distil_model.h5')","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:40:59.792166Z","iopub.execute_input":"2023-07-17T02:40:59.792910Z","iopub.status.idle":"2023-07-17T02:41:00.228831Z","shell.execute_reply.started":"2023-07-17T02:40:59.792865Z","shell.execute_reply":"2023-07-17T02:41:00.227791Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"complete_predictions = torch.cat(predictions)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:37:13.889628Z","iopub.execute_input":"2023-07-17T02:37:13.889993Z","iopub.status.idle":"2023-07-17T02:37:13.897600Z","shell.execute_reply.started":"2023-07-17T02:37:13.889963Z","shell.execute_reply":"2023-07-17T02:37:13.896497Z"},"trusted":true},"execution_count":85,"outputs":[]},{"cell_type":"code","source":"print(len(complete_predictions))\nprint(complete_predictions)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:37:37.994432Z","iopub.execute_input":"2023-07-17T02:37:37.995192Z","iopub.status.idle":"2023-07-17T02:37:38.004805Z","shell.execute_reply.started":"2023-07-17T02:37:37.995153Z","shell.execute_reply":"2023-07-17T02:37:38.003557Z"},"trusted":true},"execution_count":88,"outputs":[{"name":"stdout","text":"153164\ntensor([[0.4854, 0.4607, 0.4746, 0.4781, 0.5186, 0.5342],\n        [0.4815, 0.4629, 0.4874, 0.4888, 0.5155, 0.5277],\n        [0.4857, 0.4656, 0.4883, 0.4996, 0.4750, 0.5073],\n        ...,\n        [0.4887, 0.4598, 0.4923, 0.5085, 0.5028, 0.4958],\n        [0.4919, 0.4666, 0.4852, 0.4946, 0.4883, 0.5242],\n        [0.4839, 0.4427, 0.4885, 0.5087, 0.4988, 0.5083]], device='cuda:0')\n","output_type":"stream"}]},{"cell_type":"code","source":"final_submissions_dataset = test_data.copy()\nfinal_submissions_dataset.drop(\"comment_text\", inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:45:41.715982Z","iopub.execute_input":"2023-07-17T02:45:41.716392Z","iopub.status.idle":"2023-07-17T02:45:41.763817Z","shell.execute_reply.started":"2023-07-17T02:45:41.716359Z","shell.execute_reply":"2023-07-17T02:45:41.762402Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"for i,name in enumerate(label_columns):\n\n    final_submissions_dataset[name] = complete_predictions[:, i].cpu()\n    #final_submissions_dataset.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:46:19.773609Z","iopub.execute_input":"2023-07-17T02:46:19.773981Z","iopub.status.idle":"2023-07-17T02:46:19.786994Z","shell.execute_reply.started":"2023-07-17T02:46:19.773949Z","shell.execute_reply":"2023-07-17T02:46:19.785895Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"code","source":"final_submissions_dataset","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:46:42.327387Z","iopub.execute_input":"2023-07-17T02:46:42.328811Z","iopub.status.idle":"2023-07-17T02:46:42.347311Z","shell.execute_reply.started":"2023-07-17T02:46:42.328767Z","shell.execute_reply":"2023-07-17T02:46:42.345468Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"                      id     toxic  severe_toxic   obscene    threat  \\\n0       00001cee341fdb12  0.485438      0.460746  0.474592  0.478135   \n1       0000247867823ef7  0.481509      0.462905  0.487369  0.488781   \n2       00013b17ad220c46  0.485672      0.465597  0.488295  0.499621   \n3       00017563c3f7919a  0.476427      0.461015  0.485920  0.488724   \n4       00017695ad8997eb  0.482547      0.449387  0.475115  0.486569   \n...                  ...       ...           ...       ...       ...   \n153159  fffcd0960ee309b5  0.486624      0.463431  0.470220  0.488230   \n153160  fffd7a9a6eb32c16  0.485365      0.458224  0.485186  0.485934   \n153161  fffda9e8d6fafa9e  0.488651      0.459759  0.492317  0.508490   \n153162  fffe8f1340a79fc2  0.491879      0.466649  0.485157  0.494597   \n153163  ffffce3fb183ee80  0.483904      0.442695  0.488513  0.508724   \n\n          insult  identity_hate  \n0       0.518604       0.534152  \n1       0.515530       0.527711  \n2       0.475014       0.507290  \n3       0.496532       0.520164  \n4       0.510244       0.520836  \n...          ...            ...  \n153159  0.505266       0.520939  \n153160  0.495648       0.531145  \n153161  0.502825       0.495790  \n153162  0.488288       0.524162  \n153163  0.498821       0.508270  \n\n[153164 rows x 7 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>toxic</th>\n      <th>severe_toxic</th>\n      <th>obscene</th>\n      <th>threat</th>\n      <th>insult</th>\n      <th>identity_hate</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00001cee341fdb12</td>\n      <td>0.485438</td>\n      <td>0.460746</td>\n      <td>0.474592</td>\n      <td>0.478135</td>\n      <td>0.518604</td>\n      <td>0.534152</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0000247867823ef7</td>\n      <td>0.481509</td>\n      <td>0.462905</td>\n      <td>0.487369</td>\n      <td>0.488781</td>\n      <td>0.515530</td>\n      <td>0.527711</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00013b17ad220c46</td>\n      <td>0.485672</td>\n      <td>0.465597</td>\n      <td>0.488295</td>\n      <td>0.499621</td>\n      <td>0.475014</td>\n      <td>0.507290</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00017563c3f7919a</td>\n      <td>0.476427</td>\n      <td>0.461015</td>\n      <td>0.485920</td>\n      <td>0.488724</td>\n      <td>0.496532</td>\n      <td>0.520164</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00017695ad8997eb</td>\n      <td>0.482547</td>\n      <td>0.449387</td>\n      <td>0.475115</td>\n      <td>0.486569</td>\n      <td>0.510244</td>\n      <td>0.520836</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>153159</th>\n      <td>fffcd0960ee309b5</td>\n      <td>0.486624</td>\n      <td>0.463431</td>\n      <td>0.470220</td>\n      <td>0.488230</td>\n      <td>0.505266</td>\n      <td>0.520939</td>\n    </tr>\n    <tr>\n      <th>153160</th>\n      <td>fffd7a9a6eb32c16</td>\n      <td>0.485365</td>\n      <td>0.458224</td>\n      <td>0.485186</td>\n      <td>0.485934</td>\n      <td>0.495648</td>\n      <td>0.531145</td>\n    </tr>\n    <tr>\n      <th>153161</th>\n      <td>fffda9e8d6fafa9e</td>\n      <td>0.488651</td>\n      <td>0.459759</td>\n      <td>0.492317</td>\n      <td>0.508490</td>\n      <td>0.502825</td>\n      <td>0.495790</td>\n    </tr>\n    <tr>\n      <th>153162</th>\n      <td>fffe8f1340a79fc2</td>\n      <td>0.491879</td>\n      <td>0.466649</td>\n      <td>0.485157</td>\n      <td>0.494597</td>\n      <td>0.488288</td>\n      <td>0.524162</td>\n    </tr>\n    <tr>\n      <th>153163</th>\n      <td>ffffce3fb183ee80</td>\n      <td>0.483904</td>\n      <td>0.442695</td>\n      <td>0.488513</td>\n      <td>0.508724</td>\n      <td>0.498821</td>\n      <td>0.508270</td>\n    </tr>\n  </tbody>\n</table>\n<p>153164 rows × 7 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"final_submissions_dataset.to_csv('submission_distil_final.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-17T02:47:13.997879Z","iopub.execute_input":"2023-07-17T02:47:13.998996Z","iopub.status.idle":"2023-07-17T02:47:15.464334Z","shell.execute_reply.started":"2023-07-17T02:47:13.998948Z","shell.execute_reply":"2023-07-17T02:47:15.463174Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}